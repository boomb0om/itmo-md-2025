# Отчёт по проекту: Crypto Data Pipeline

**Курс: Управление данными**  

**ИТМО, 2025**

---

## 1. Executive Summary

В рамках первого задания курса «Управление данными» была разработана полноценная система сбора и обработки данных о криптовалютах. Проект реализует концепцию ELT (Extract-Load-Transform) и включает в себя не только базовые требования задания, но и начало реализаций компонент для  автоматизированного сбора данных из внешних источников.

**Основные достижения:**

- Развёрнута полноценная инфраструктура на сервере ([http://213.171.30.141:8080](http://213.171.30.141:8080))
- Настроен EL-процесс в Apache Airflow с двумя DAG для автоматизации
- Данные успешно переносятся из MongoDB в PostgreSQL аналитическое хранилище
- Создан репозиторий с полной документацией и docker-compose конфигурацией
- Начата реализация сервиса сбора данных с FastAPI для взаимодействия с Binance API и CryptoPanic

---

## 2. Бизнес-кейс и постановка задачи

### 2.1. Описание кейса

**Тема:** Влияние новостей на криптовалюты

**Бизнес-задача:** Помочь трейдерам принимать решения на основе новостей путём анализа корреляции новостей и волатильности рынка криптовалют.

**Уровень сложности:** **Сложный**

### 2.2. Источники данных

Для решения задачи будут использоваться два основных источника данных:

- **Binance API:** исторические данные о ценах криптовалют (klines/свечи), включая цены открытия, закрытия, максимум, минимум и объёмы торгов
- **CryptoPanic RSS:** новостные ленты о криптовалютах из различных источников с временными метками публикации

### 2.3. Ключевые задачи анализа

- Корреляция новостей и волатильности рынка
- Анализ тональности текстов новостей (sentiment analysis)

---

## 3. Архитектура системы

Проект построен по модульной архитектуре и включает три основных компонента, каждый из которых может работать независимо:

### 3.1. Data Collection Service (app/)

**Назначение:** FastAPI приложение для сбора данных из внешних источников

**Технологический стек:**
- FastAPI — веб-фреймворк для создания REST API
- MongoDB — операционная база данных (OLTP)
- uv — современный менеджер пакетов Python
- pydantic-settings — управление конфигурацией
- aiologger — асинхронное логирование

**Основные API endpoints:**
- **GET /api/fetch-klines:** получение исторических данных свечей (klines) с Binance API
- **GET /api/fetch-news:** получение новостей из CryptoPanic RSS-лент
- **GET /health:** проверка состояния сервиса

**Хранение данных:** Все собранные данные сохраняются в MongoDB в коллекции klines, news и requests_log с полным логированием запросов.

### 3.2. Data Warehouse (dwh/)

**Назначение:** PostgreSQL база данных для хранения аналитических данных (OLAP)

**Технологический стек:**
- PostgreSQL 13 — реляционная СУБД для аналитики

**Структура данных:** Данные хранятся в схеме raw с таблицами:
- **raw.klines:** исторические данные свечей криптовалют
- **raw.news:** новости с метаданными
- **raw.requests_log:** логи всех запросов к API

### 3.3. Orchestration Service (airflow/)

**Назначение:** Apache Airflow для оркестрации и автоматизации процессов сбора и переноса данных

**Технологический стек:**
- Apache Airflow 2.10.5 — платформа для оркестрации рабочих процессов

**Реализованные DAG (направленные ациклические графы):**
- **collect_data:** автоматический сбор данных каждый час, выполняет запросы к FastAPI сервису для получения свежих данных о криптовалютах и новостях
- **el_process:** EL-процесс каждые 6 часов, извлекает данные из MongoDB и загружает их в PostgreSQL с трансформацией структуры

---

## 4. Детали реализации

### 4.1. EL-процесс (Extract-Load)

EL-процесс реализован в Airflow DAG el_process и выполняет следующие операции:

**Этап Extract (извлечение):**
- Подключение к MongoDB
- Извлечение документов из коллекций klines, news и requests_log
- Фильтрация по временным меткам для инкрементальной загрузки

**Этап Load (загрузка):**
- Трансформация структуры данных (упрощение, сериализация JSON полей)
- Создание таблиц в PostgreSQL при необходимости
- Bulk insert данных в схему raw
- Обработка дубликатов с использованием upsert операций

### 4.2. Структура данных

Данные в PostgreSQL организованы следующим образом:

| Таблица | Описание | Ключевые поля |
|---------|----------|---------------|
| **raw.klines** | Исторические данные свечей криптовалют | symbol, interval, open_time, open_price, close_price, high, low, volume |
| **raw.news** | Новости о криптовалютах | title, url, published_at, source, currencies |
| **raw.requests_log** | Логи запросов к API | endpoint, method, status_code, response_time, timestamp |

### 4.3. Инфраструктура и развертывание

**Развертывание:** Вся система развернута на сервере с использованием Docker и Docker Compose

**Сервисы:**
- FastAPI приложение (app) — порт 8000
- MongoDB — порт 27017
- PostgreSQL — порт 5433
- Airflow WebUI — порт 8080 (http://213.171.30.141:8080)
- Airflow Scheduler — фоновый процесс

**Конфигурация:** Все настройки управляются через .env файлы для каждого модуля, что обеспечивает гибкость и безопасность конфигурации.

---

## 5. Результаты и артефакты

### 5.1. Достигнутые результаты

- **Выбран кейс:** «Влияние новостей на криптовалюты» — сложный кейс с реальными бизнес-применениями
- **Создан Git-репозиторий:** https://github.com/boomb0om/itmo-md-2025 с полной документацией
- **Развернуто рабочее приложение:** FastAPI сервис для сбора данных из Binance и CryptoPanic
- **Реализован EL-процесс:** автоматический перенос данных из MongoDB в PostgreSQL
- **Развернут Airflow:** http://213.171.30.141:8080 с рабочими DAG для автоматизации

### 5.2. Автоматизация процессов

В системе реализованы два автоматизированных процесса:

- **Сбор данных (collect_data DAG):** выполняется каждый час, обеспечивает актуальность данных о рынке и новостях
- **EL-процесс (el_process DAG):** выполняется каждые 6 часов, обеспечивает регулярное обновление аналитического хранилища

---

## 6. Технологический стек

| Категория | Технологии |
|-----------|------------|
| **Backend** | Python 3.11+, FastAPI, pydantic-settings, aiologger |
| **Базы данных** | MongoDB (OLTP), PostgreSQL 13 (OLAP) |
| **Оркестрация** | Apache Airflow 2.10.5 |
| **Инфраструктура** | Docker, Docker Compose |
| **Инструменты** | uv (пакетный менеджер), Git |

---

## 7. Дальнейшие шаги

Проект выполнен с опережением базовых требований Задания 1. Следующие шаги развития включают:

### 7.1. Задание 2: Трансформация данных с DBT

- Создание моделей DBT для трансформации сырых данных
- Расчёт метрик волатильности для криптовалют
- Построение витрин данных для анализа корреляций

### 7.2. Аналитические задачи

- Реализация sentiment analysis для текстов новостей
- Построение корреляций между тональностью новостей и волатильностью
- Создание дашбордов для визуализации результатов

### 7.3. Улучшения инфраструктуры

- Добавление мониторинга с использованием Elementary
- Настройка алертов о проблемах в пайплайнах
- Оптимизация производительности запросов

---

## 8. Заключение

В рамках первого задания курса «Управление данными» была успешно реализована полноценная система сбора и обработки данных о криптовалютах. Проект превосходит базовые требования задания, включая не только EL-процесс, но и автоматизированный сбор данных из внешних источников через REST API.

Система построена с использованием современного технологического стека, следует лучшим практикам разработки (модульная архитектура, контейнеризация, автоматизация) и готова к дальнейшему развитию для решения аналитических задач по корреляции новостей и волатильности криптовалют.

Все компоненты системы развернуты и работают в production-режиме, что подтверждается доступностью Airflow WebUI по адресу **http://213.171.30.141:8080** и регулярным выполнением DAG по расписанию.

---

## 9. Приложения

### 9.1. Ссылки на ресурсы

- **GitHub репозиторий:** [https://github.com/boomb0om/itmo-md-2025](https://github.com/boomb0om/itmo-md-2025)
- **Airflow WebUI:** [http://213.171.30.141:8080](http://213.171.30.141:8080) (credentials: airflow/airflow)
- **FastAPI Documentation:** [http://213.171.30.141:8000/docs](http://213.171.30.141:8000/docs)

### 9.2. Структура репозитория

Основные директории:

- **app/:** FastAPI приложение для сбора данных
- **dwh/:** Data Warehouse конфигурация
- **airflow/:** Airflow DAG и конфигурация
- **context/:** Описание заданий курса
- **course/:** Примеры и референсные материалы

### 9.3. Примеры команд

**Запуск всей системы:**
```bash
docker-compose up -d
```

**Просмотр логов:**
```bash
docker-compose logs -f airflow-scheduler
```

**Подключение к PostgreSQL:**
```bash
psql -h localhost -p 5433 -U analytics -d analytics
```

---

*Отчёт подготовлен в рамках курса «Управление данными»*  
*ИТМО, 2025*
