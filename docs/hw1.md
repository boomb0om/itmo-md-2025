# Отчёт по проекту: Crypto Data Pipeline

**Курс: Управление данными**  

**ИТМО, 2025**

---

В рамках первого задания курса «Управление данными» была разработана система сбора и обработки данных о криптовалютах. Первое задание включает в себя настроенную инфраструктуру, базовый EL-пайплайн и CI/CD с деплоем всей системы.

**Итоги задания:**

- Airflow развернут на сервере ([http://213.171.30.141:8080](http://213.171.30.141:8080))
- Настроен EL-процесс в Apache Airflow с двумя DAG
- Данные успешно переносятся из MongoDB в PostgreSQL аналитическое хранилище
- Создан репозиторий с полной документацией и docker compose конфигурацией
- Начата реализация сервиса сбора данных с FastAPI для взаимодействия с Binance API и CryptoPanic

---

## 2. Бизнес-кейс и постановка задачи

### 2.1. Описание кейса

**Тема:** Влияние новостей на криптовалюты

**Бизнес-задача:** Помочь трейдерам принимать решения на основе новостей путём анализа корреляции новостей и волатильности рынка криптовалют.

### 2.2. Источники данных

Для решения задачи будут использоваться два основных источника данных:

- **Binance API:** исторические данные о ценах криптовалют, включая цены открытия, закрытия, максимум, минимум и объёмы торгов
- **CryptoPanic RSS:** новостные ленты о криптовалютах из различных источников с временными метками публикации

### 2.3. Ключевые задачи анализа

- Корреляция новостей и волатильности рынка
- Анализ тональности текстов новостей (sentiment analysis)

---

## 3. Архитектура системы

Проект построен по модульной архитектуре и включает три основных компонента, каждый из которых может работать независимо:

### 3.1. Сервис сбора данных (app/)

FastAPI приложение для сбора данных из внешних источников

**Технологический стек:**
- FastAPI — веб-фреймворк для создания REST API
- MongoDB — операционная база данных (OLTP)
- uv — современный менеджер пакетов Python
- pydantic-settings — управление конфигурацией
- aiologger — асинхронное логирование

**Хранение данных:** Все собранные данные сохраняются в MongoDB в коллекциях klines, news и requests_log с полным логированием запросов.

### 3.2. DWH (dwh/)

PostgreSQL база данных для хранения аналитических данных

**Структура данных:** Данные хранятся в схеме raw с таблицами:
- **raw.klines:** исторические данные свечей криптовалют
- **raw.news:** новости с метаданными
- **raw.requests_log:** логи всех запросов к API

### 3.3. Оркестрация (airflow/)

Apache Airflow для оркестрации и автоматизации процессов сбора и переноса данных

**Реализованные DAG (направленные ациклические графы):**
- **collect_data:** автоматический сбор данных каждый час, выполняет запросы к FastAPI сервису для получения свежих данных о криптовалютах и новостях
- **el_process:** EL-процесс каждые 6 часов, извлекает данные из MongoDB и загружает их в PostgreSQL с трансформацией структуры

---

## 4. Детали реализации

### 4.1. EL-процесс (Extract-Load)

EL-процесс реализован в Airflow DAG el_process и выполняет следующие операции:

**Этап Extract (извлечение):**
- Подключение к MongoDB
- Извлечение документов из коллекций klines, news и requests_log
- Фильтрация по временным меткам для инкрементальной загрузки

**Этап Load (загрузка):**
- Трансформация структуры данных (упрощение, сериализация JSON полей)
- Создание таблиц в PostgreSQL при необходимости
- Bulk insert данных в схему raw
- Обработка дубликатов с использованием upsert операций

### 4.2. Структура данных

Данные в PostgreSQL организованы следующим образом:

| Таблица | Описание | Ключевые поля |
|---------|----------|---------------|
| **raw.klines** | Исторические данные свечей криптовалют | symbol, interval, open_time, open_price, close_price, high, low, volume |
| **raw.news** | Новости о криптовалютах | title, url, published_at, source, currencies |
| **raw.requests_log** | Логи запросов к API | endpoint, method, status_code, response_time, timestamp |

### 4.3. Инфраструктура и развертывание

**Развертывание:** Вся система развернута на сервере с использованием Docker и Docker Compose

**Сервисы:**
- PostgreSQL - целевая база для аналитических данных (порт 5433)
- MongoDB - источник данных для "продового приложения" (порт 27017)
- Apache Airflow - оркестрация EL-процессов (порт 8080)
- crypto-app - Python-приложение для сбора данных о криптовалютах и новостей

---

## 5. Результаты и артефакты

### 5.1. Достигнутые результаты

- **Выбран кейс:** «Влияние новостей на криптовалюты» — сложный кейс с реальными бизнес-применениями
- **Создан Git-репозиторий:** https://github.com/boomb0om/itmo-md-2025 с полной документацией
- **Развернуто рабочее приложение:** FastAPI сервис для сбора данных из Binance и CryptoPanic
- **Реализован EL-процесс:** автоматический перенос данных из MongoDB в PostgreSQL
- **Развернут Airflow:** http://213.171.30.141:8080 с рабочими DAG для автоматизации

### 5.2. Автоматизация процессов

В системе реализованы два автоматизированных процесса:

- **Сбор данных (collect_data DAG):** выполняется каждый час, обеспечивает актуальность данных о рынке и новостях
- **EL-процесс (el_process DAG):** выполняется каждые 6 часов, обеспечивает регулярное обновление аналитического хранилища

## 6. Результаты

### 6.1. Ссылки

- **GitHub репозиторий:** [https://github.com/boomb0om/itmo-md-2025](https://github.com/boomb0om/itmo-md-2025)
- **Airflow WebUI:** [http://213.171.30.141:8080](http://213.171.30.141:8080) (credentials: airflow/airflow)
- **FastAPI Documentation:** [http://213.171.30.141:8000/docs](http://213.171.30.141:8000/docs)

### 9.2. Артефакты

Основные директории:

✅ Git-репозиторий с кодом проекта
✅ Python-приложение для записи в MongoDB
✅ EL-процесс в Airflow для переноса данных в PostgreSQL
✅ Рабочая инфраструктура на docker compose
✅ CI/CD с деплоем на сервер